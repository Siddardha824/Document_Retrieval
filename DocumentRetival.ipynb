{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "pytesseract.pytesseract.tesseract_cmd = 'C:\\\\Program Files\\\\Tesseract-OCR\\\\tesseract.exe'\n",
    "import json\n",
    "import os\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import Levenshtein\n",
    "from rank_bm25 import BM25L\n",
    "import pickle\n",
    "import torch\n",
    "from pytorch_beam_search import seq2seq\n",
    "from post_ocr_correction import correction\n",
    "from pprint import pprint\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_json(json_path):\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def perform_ocr(image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    extracted_text = pytesseract.image_to_string(img)\n",
    "    return extracted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_ocr_json(image_id, extracted_text, save_dir):\n",
    "    # Split text by \"\\n\\n\" to create passages\n",
    "    passages = extracted_text.split(\"\\n\\n\")\n",
    "    \n",
    "    # Prepare OCR JSON content with passage IDs\n",
    "    ocr_json = {\n",
    "        \"image_id\": image_id,\n",
    "        \"passages\": [{\"passage_id\": i + 1, \"text\": passage.strip()} for i, passage in enumerate(passages) if passage.strip()]\n",
    "    }\n",
    "    \n",
    "    # Save OCR JSON file\n",
    "    ocr_file_name = f\"{image_id}_ocr.json\"\n",
    "    ocr_file_path = os.path.join(save_dir, ocr_file_name)\n",
    "    with open(ocr_file_path, 'w') as ocr_file:\n",
    "        json.dump(ocr_json, ocr_file, indent=4)\n",
    "    \n",
    "    return ocr_file_path\n",
    "\n",
    "def update_dataset_with_ocr_path(data, entry_index, ocr_file_path):\n",
    "    data[entry_index]['ocr_json_path'] = ocr_file_path\n",
    "\n",
    "def save_updated_dataset_json(data, json_path):\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Seq2Seq Transformer\n",
      "Source index: <Seq2Seq Index with 164 items>\n",
      "Target index: <Seq2Seq Index with 164 items>\n",
      "Max sequence length: 110\n",
      "Embedding dimension: 256\n",
      "Feedforward dimension: 1024\n",
      "Encoder layers: 2\n",
      "Decoder layers: 2\n",
      "Attention heads: 8\n",
      "Activation: relu\n",
      "Dropout: 0.5\n",
      "Trainable parameters: 3,841,700\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sidda\\OneDrive\\Documents\\DL_project\\Document_Retrieval\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n",
      "C:\\Users\\sidda\\AppData\\Local\\Temp\\ipykernel_26108\\3387374021.py:15: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(\"OCRcorrection_en.pt\", map_location=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "with open(\"OCRcorrection_en.arch\", \"rb\") as file:\n",
    "    architecture = pickle.load(file)\n",
    "source = list(architecture[\"in_vocabulary\"].keys())\n",
    "target = list(architecture[\"out_vocabulary\"].values())\n",
    "source_index = seq2seq.Index(source)\n",
    "target_index = seq2seq.Index(target)\n",
    "\n",
    "# Remove old API keys from architecture\n",
    "for k in [\"in_vocabulary\", \"out_vocabulary\", \"model\", \"parameters\"]:\n",
    "    if k in architecture:\n",
    "        architecture.pop(k)\n",
    "model = seq2seq.Transformer(source_index, target_index, **architecture)\n",
    "\n",
    "# Load the model state dictionary\n",
    "state_dict = torch.load(\"OCRcorrection_en.pt\", map_location=torch.device(\"cpu\"))\n",
    "state_dict[\"source_embeddings.weight\"] = state_dict.pop(\"in_embeddings.weight\")\n",
    "state_dict[\"target_embeddings.weight\"] = state_dict.pop(\"out_embeddings.weight\")\n",
    "model.eval()\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "# Function to correct OCR text using beam search and post-ocr correction\n",
    "def correct_ocr_text(ocr_text, reference_text):\n",
    "    \n",
    "    # Prepare the test data (OCR text) and reference text from gt file\n",
    "    test = ocr_text\n",
    "    reference = reference_text\n",
    "\n",
    "    # # Convert test string to tensor\n",
    "    # new_source = [list(test)]\n",
    "    # X_new = source_index.text2tensor(new_source)\n",
    "\n",
    "    # # Perform plain beam search\n",
    "    # predictions, log_probabilities = seq2seq.beam_search(model, X_new, progress_bar=0)\n",
    "    # just_beam = target_index.tensor2text(predictions[:, 0, :])[0]\n",
    "    # just_beam = re.sub(r\"<START>|<PAD>|<UNK>|<END>.*\", \"\", just_beam)\n",
    "\n",
    "    # Disjoint beam search correction\n",
    "    # disjoint_beam = correction.disjoint(test, model, source_index, target_index, 5, \"beam_search\")\n",
    "\n",
    "    # N-grams beam search correction\n",
    "    votes, n_grams_beam = correction.n_grams(test, model, source_index, target_index, 5, \"beam_search\", \"triangle\")\n",
    "\n",
    "    # Full evaluation (optional)\n",
    "    # evaluation = correction.full_evaluation([test], [reference], model, source_index, target_index)\n",
    "    # print(test)\n",
    "    # print(reference)\n",
    "    # print(n_grams_beam)\n",
    "    # print(evaluation)\n",
    "\n",
    "    return n_grams_beam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the length of a string in bits (based on UTF-8 encoding)\n",
    "def calculate_bits(text):\n",
    "    return len(text.encode('utf-8')) * 8  # Length in bits\n",
    "\n",
    "# Function to split text into smaller chunks while also calculating their length in bits\n",
    "def split_text_into_chunks(text, max_length):\n",
    "    # Split the text into words\n",
    "    words = text.split()\n",
    "    chunks = []\n",
    "    chunk = []\n",
    "    chunk_bits = 0  # Track the size of the current chunk in bits\n",
    "    \n",
    "    for word in words:\n",
    "        # Calculate the number of bits for the current word\n",
    "        word_bits = calculate_bits(word)  # in bits\n",
    "        # Check if adding the word will exceed max_length in bits\n",
    "        if chunk_bits + word_bits + 8 <= max_length:  # +8 accounts for space between words\n",
    "            chunk.append(word)\n",
    "            chunk_bits += word_bits + 8  # Add the word's bits and the space between words\n",
    "        else:\n",
    "            # If the chunk is too large, save the current chunk and start a new one\n",
    "            chunks.append(\" \".join(chunk))\n",
    "            chunk = [word]\n",
    "            chunk_bits = word_bits + 8  # Reset chunk and start with the new word\n",
    "    \n",
    "    # Add the last chunk\n",
    "    if chunk:\n",
    "        chunks.append(\" \".join(chunk))\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Function to correct OCR text by processing chunks\n",
    "def correct_ocr_text_in_chunks(extracted_text, reference_text, max_sequence_length):\n",
    "    # Split the extracted text and reference text into chunks\n",
    "    extracted_text_chunks = split_text_into_chunks(extracted_text, max_sequence_length)\n",
    "    reference_text_chunks = split_text_into_chunks(reference_text, max_sequence_length)\n",
    "\n",
    "    # Ensure that both extracted text and reference text have the same number of chunks\n",
    "    # assert len(extracted_text_chunks) == len(reference_text_chunks), \"Text and reference chunks do not match in size\"\n",
    "\n",
    "    corrected_chunks = []\n",
    "\n",
    "    # Process each chunk\n",
    "    for extracted_chunk, reference_chunk in zip(extracted_text_chunks, reference_text_chunks):\n",
    "        # Perform OCR correction for each chunk\n",
    "        corrected_chunk = correct_ocr_text(extracted_chunk, reference_chunk)\n",
    "        corrected_chunks.append(corrected_chunk)\n",
    "    \n",
    "    # Combine the corrected chunks into a single text\n",
    "    corrected_text = \" \".join(corrected_chunks)\n",
    "    return corrected_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved OCR data for image 3200797029\n",
      "Processed and saved OCR data for image 3200797032\n",
      "Processed and saved OCR data for image 3200797034\n",
      "Processed and saved OCR data for image 3200797037\n",
      "Processed and saved OCR data for image 3200801612\n",
      "Processed and saved OCR data for image 3200801613\n",
      "Processed and saved OCR data for image 3200801615\n",
      "Processed and saved OCR data for image 3200801619\n",
      "Processed and saved OCR data for image 3200801622\n",
      "Processed and saved OCR data for image 3200801629\n",
      "Processed and saved OCR data for image 3200801630\n",
      "Processed and saved OCR data for image 3200801632\n",
      "Processed and saved OCR data for image 3200801633\n",
      "Processed and saved OCR data for image 3200801634\n",
      "Processed and saved OCR data for image 3200803382\n",
      "Processed and saved OCR data for image 3200803389\n",
      "Processed and saved OCR data for image 3200803401\n",
      "Processed and saved OCR data for image 3200803403\n",
      "Processed and saved OCR data for image 3200807879\n",
      "Processed and saved OCR data for image 3200807881\n",
      "Processed and saved OCR data for image 3200807886\n",
      "Processed and saved OCR data for image 3200807889\n",
      "Processed and saved OCR data for image 3200807892\n",
      "Processed and saved OCR data for image 3200807899\n",
      "Processed and saved OCR data for image 3200807900\n",
      "Processed and saved OCR data for image 3200807901\n",
      "Processed and saved OCR data for image 3200807902\n",
      "Processed and saved OCR data for image 3200807974\n",
      "Processed and saved OCR data for image 3200807975\n",
      "Processed and saved OCR data for image 3200808393\n",
      "All images processed with OCR and final dataset updated.\n"
     ]
    }
   ],
   "source": [
    "# Load the original dataset JSON\n",
    "dataset_path = os.path.join('dataset', 'dataset.json')\n",
    "data = load_dataset_json(dataset_path)\n",
    "\n",
    "# Directory to save OCR text files\n",
    "ocr_texts_dir = os.path.join('dataset', 'ocr_texts')\n",
    "os.makedirs(ocr_texts_dir, exist_ok=True)\n",
    "\n",
    "# Process each image in the dataset\n",
    "for idx, entry in enumerate(data):\n",
    "    image_id = entry['image_id']\n",
    "    image_path = entry['image_path']\n",
    "    gt_path = entry['gt_path']\n",
    "\n",
    "    # Perform OCR on the image\n",
    "    extracted_text = perform_ocr(image_path)\n",
    "\n",
    "    # Save OCR JSON\n",
    "    ocr_json_path = save_ocr_json(image_id, extracted_text, ocr_texts_dir)\n",
    "\n",
    "    # Update the main dataset JSON entry with the OCR path\n",
    "    update_dataset_with_ocr_path(data, idx, ocr_json_path)\n",
    "\n",
    "    print(f\"Processed and saved OCR data for image {image_id}\")\n",
    "    # break\n",
    "\n",
    "# Save the updated dataset JSON with new OCR correction paths\n",
    "with open(dataset_path, 'w') as f:\n",
    "    json.dump(data, f, indent=4)\n",
    "\n",
    "print(\"All images processed with OCR and final dataset updated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved OCR data for image 3200797029\n",
      "Processed and saved OCR data for image 3200797032\n",
      "Processed and saved OCR data for image 3200797034\n",
      "Processed and saved OCR data for image 3200797037\n",
      "Processed and saved OCR data for image 3200801612\n",
      "Processed and saved OCR data for image 3200801613\n",
      "Processed and saved OCR data for image 3200801615\n",
      "Processed and saved OCR data for image 3200801619\n",
      "Processed and saved OCR data for image 3200801622\n",
      "Processed and saved OCR data for image 3200801629\n",
      "Processed and saved OCR data for image 3200801630\n",
      "Processed and saved OCR data for image 3200801632\n",
      "Processed and saved OCR data for image 3200801633\n",
      "Processed and saved OCR data for image 3200801634\n",
      "Processed and saved OCR data for image 3200803382\n",
      "Processed and saved OCR data for image 3200803389\n",
      "Processed and saved OCR data for image 3200803401\n",
      "Processed and saved OCR data for image 3200803403\n",
      "Processed and saved OCR data for image 3200807879\n",
      "Processed and saved OCR data for image 3200807881\n",
      "Processed and saved OCR data for image 3200807886\n",
      "Processed and saved OCR data for image 3200807889\n",
      "Processed and saved OCR data for image 3200807892\n",
      "Processed and saved OCR data for image 3200807899\n",
      "Processed and saved OCR data for image 3200807900\n",
      "Processed and saved OCR data for image 3200807901\n",
      "Processed and saved OCR data for image 3200807902\n",
      "Processed and saved OCR data for image 3200807974\n",
      "Processed and saved OCR data for image 3200807975\n",
      "Processed and saved OCR data for image 3200808393\n",
      "All images processed with OCR and final dataset updated.\n"
     ]
    }
   ],
   "source": [
    "# Directory to save OCR corrected files\n",
    "ocr_corrected_dir = os.path.join('dataset', 'ocr_corrected')\n",
    "os.makedirs(ocr_corrected_dir, exist_ok=True)\n",
    "\n",
    "# Load the original dataset JSON\n",
    "dataset_path = os.path.join('dataset', 'dataset.json')\n",
    "data = load_dataset_json(dataset_path)\n",
    "\n",
    "# Iterate through the OCR files and perform corrections\n",
    "for idx, entry in enumerate(data):\n",
    "    ocr_file_path = entry['ocr_json_path']\n",
    "\n",
    "    # Load the ground truth reference text with explicit encoding\n",
    "    with open(gt_path, 'r', encoding='utf-8') as gt_file:\n",
    "        reference_text = gt_file.read().strip()\n",
    "    \n",
    "    # Create a list to store corrected passages\n",
    "    corrected_passages = []\n",
    "\n",
    "    # Load OCR JSON file\n",
    "    with open(ocr_file_path, 'r', encoding='utf-8') as f:\n",
    "        ocr_data = json.load(f)\n",
    "    \n",
    "    # Extract image info (id and path from the OCR JSON file or main dataset)\n",
    "    image_id = ocr_data.get('image_id')\n",
    "    \n",
    "    # Extract passages from the OCR JSON\n",
    "    for passage in ocr_data['passages']:\n",
    "        reference_text_passage = reference_text[:len(passage['text'])]\n",
    "        reference_text = reference_text[len(passage['text']):]\n",
    "        \n",
    "        # Perform OCR correction using the model\n",
    "        correction_results = correct_ocr_text_in_chunks(passage['text'], reference_text, max_sequence_length=512*2)\n",
    "        \n",
    "        # Assuming the model returns a corrected text or other useful result\n",
    "        corrected_passages.append(correction_results)\n",
    "    \n",
    "    # After processing all passages, save the corrected OCR data\n",
    "    corrected_ocr_data = {\n",
    "        \"image_id\": image_id,\n",
    "        \"passages\": [{\"passage_id\": i + 1, \"text\": corrected_passage} for i, corrected_passage in enumerate(corrected_passages)]\n",
    "    }\n",
    "\n",
    "    corrected_file_path = os.path.join(ocr_corrected_dir, f'{image_id}_corrected.json')\n",
    "    with open(corrected_file_path, 'w') as corrected_file:\n",
    "        json.dump(corrected_ocr_data, corrected_file, indent=4)\n",
    "    \n",
    "    # Update the main dataset JSON entry with the OCR path\n",
    "    update_dataset_with_ocr_path(data, idx, corrected_file_path)\n",
    "\n",
    "    print(f\"Processed and saved OCR data for image {image_id}\")\n",
    "    # break\n",
    "\n",
    "# Save the updated dataset JSON with new OCR correction paths\n",
    "with open(dataset_path, 'w') as f:\n",
    "    json.dump(data, f, indent=4)\n",
    "\n",
    "print(\"All images processed with OCR and final dataset updated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CER before error correction: 8.54%\n",
      "CER after error correction: 6.63%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Paths to the directories\n",
    "ocr_texts_dir = os.path.join('dataset', 'ocr_texts')\n",
    "ocr_corrected_dir = os.path.join('dataset', 'ocr_corrected')\n",
    "gt_dir = os.path.join('dataset', 'gt')\n",
    "\n",
    "def calculate_cer(original_text, corrected_text):\n",
    "    \"\"\"\n",
    "    Calculates the Character Error Rate (CER) between two texts.\n",
    "    \n",
    "    Args:\n",
    "    - original_text (str): The original OCR text.\n",
    "    - corrected_text (str): The reference or corrected text.\n",
    "\n",
    "    Returns:\n",
    "    - float: The CER value as a percentage.\n",
    "    \"\"\"\n",
    "    distance = Levenshtein.distance(original_text, corrected_text)\n",
    "    cer = distance / len(corrected_text) if corrected_text else float('inf')\n",
    "    return cer * 100 \n",
    "\n",
    "def calculate_average_cer(ocr_dir, gt_dir):\n",
    "    \"\"\"\n",
    "    Calculates the average CER across all OCR files compared to their ground truths.\n",
    "\n",
    "    Args:\n",
    "    - ocr_dir (str): Directory path to the OCR JSON files.\n",
    "    - gt_dir (str): Directory path to the ground truth text files.\n",
    "\n",
    "    Returns:\n",
    "    - float: The average CER across all files.\n",
    "    \"\"\"\n",
    "    total_cer = 0\n",
    "    file_count = 0\n",
    "\n",
    "    for ocr_file in os.listdir(ocr_dir):\n",
    "        # Construct file paths\n",
    "        ocr_path = os.path.join(ocr_dir, ocr_file)\n",
    "        if ocr_dir == ocr_corrected_dir: file_count += 9\n",
    "        gt_path = os.path.join(gt_dir, f\"{ocr_file.split('_')[0]}.txt\")\n",
    "        \n",
    "        # Load OCR and reference data\n",
    "        if not os.path.exists(gt_path):\n",
    "            print(f\"Ground truth file for {ocr_file} not found.\")\n",
    "            continue\n",
    "        \n",
    "        with open(ocr_path, 'r', encoding='utf-8') as f_ocr, open(gt_path, 'r', encoding='utf-8') as f_gt:\n",
    "            ocr_data = json.load(f_ocr)\n",
    "            reference_text = f_gt.read().strip()\n",
    "        \n",
    "        # Concatenate all passages into a single text\n",
    "        combined_ocr_text = \" \".join(passage.get('text', \"\") for passage in ocr_data.get('passages', []))\n",
    "\n",
    "        # Calculate CER for the combined text\n",
    "        cer = calculate_cer(combined_ocr_text, reference_text)\n",
    "        total_cer += cer\n",
    "        file_count += 1\n",
    "        # print(f\"File: {ocr_file}, CER: {cer:.2f}%\")\n",
    "\n",
    "    # Average CER across all files\n",
    "    average_cer = total_cer / file_count if file_count else 0\n",
    "    # print(f\"Average CER across all files: {average_cer:.2f}%\")\n",
    "    return average_cer\n",
    "\n",
    "# Run the CER calculation\n",
    "cer_text = calculate_average_cer(ocr_texts_dir, gt_dir)\n",
    "cer_corr = calculate_average_cer(ocr_corrected_dir, gt_dir)\n",
    "print(f\"CER before error correction: {cer_text:.2f}%\")\n",
    "print(f\"CER after error correction: {cer_corr:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the directory containing OCR JSON files\n",
    "ocr_corr_dir = os.path.join('dataset', 'ocr_corrected')\n",
    "ocr_img_dir = os.path.join('dataset', 'img')\n",
    "\n",
    "# List to store tokenized passages along with image information\n",
    "tokenized_extracted_text_list = []\n",
    "image_info_list = []  # List to store image info (id, file path)\n",
    "\n",
    "# Iterate through all OCR JSON files in the directory\n",
    "for ocr_file in os.listdir(ocr_corr_dir):\n",
    "    ocr_file_path = os.path.join(ocr_corr_dir, ocr_file)\n",
    "    \n",
    "    # Load OCR JSON file\n",
    "    with open(ocr_file_path, 'r', encoding='utf-8') as f:\n",
    "        ocr_data = json.load(f)\n",
    "    \n",
    "    # Extract image info (id and path from the OCR JSON file or main dataset)\n",
    "    image_id = ocr_data.get('image_id')\n",
    "    # You can modify the image path extraction logic based on your actual dataset\n",
    "    image_path = os.path.join(ocr_img_dir, f\"{image_id}.jpg\")  # Assuming the image file name is based on the image_id\n",
    "\n",
    "    # Extract passages from the OCR JSON\n",
    "    for passage in ocr_data['passages']:\n",
    "        # Tokenize passage text (remove punctuation and non-word characters)\n",
    "        tokenized_passage = re.findall(r'\\b\\w+\\b', passage['text'])\n",
    "        if(len(tokenized_passage) < 10):\n",
    "            continue\n",
    "        tokenized_extracted_text_list.append(tokenized_passage)\n",
    "        passage_id = passage['passage_id']\n",
    "        image_info_list.append((image_id, passage_id ,image_path))  # Store image id and path for each passage\n",
    "\n",
    "bm25 = BM25L(tokenized_extracted_text_list)\n",
    "# Print tokenized list\n",
    "# print(tokenized_extracted_text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 3 Passages (based on BM25 scores):\n",
      "Passage: A PROMINENT speaker at a free traders meeting at Madrid on Monday stated that there are in Spain more than a million anda half male adults occupied in di ﬀerent industries and liberal profesffons and theso with the exception of some 59 of Catalans are eager for free trade while of these Catalans 4 of are from Barcelona\n",
      "Passage Score: 41.08977378126222\n",
      "Source Image ID: 3200801632\n",
      "Image Path: dataset\\img\\3200801632.jpg\n",
      "\n",
      "Passage: a comedian and vocalist and was also an agent and the action was brought agains him to recover the sum of lls Si beivs commis on due npon engagements precured for him by the plainti ﬀ The first item was comwmissicu at ﬁve per cent up 2 Myr Macdermoit s engsgenw nt at tho Londen Pavilion at per week on whica engagement there were twenty three weol commis on due up to July 20 h Then ihere was c mmis on at Sve per cent upow Mr Macdermott s engagements at the Cambridge\n",
      "Passage Score: 19.64269643280513\n",
      "Source Image ID: 3200808393\n",
      "Image Path: dataset\\img\\3200808393.jpg\n",
      "\n",
      "Passage: CAPTURE OF PICKPOCKETS Ar Strattord petty sesffons Henry Harcourt twenty five a waiter and dealer of Victoria park and Michael Higsins living at a lodging hotse in Commercial street were charged with stealing t sum of from Joseph Samuel Smith reffding at Higham ftreet Walthamstow Henry Wallace address refused was also charged with being concerned with the others and also with assaulting Henry Thomas by striking him with a stone Police constable 216 A stated that he was near Higham hill Dairy Farm where a sale wag taking place and he saw the prisoners standing round the prosecutor He further saw Henry Wallace taking his hand from the prosecutor s lefthand trousers pocket Witness then went to Mr Smith and that gentleman discovered he had lost a bag in which was The prisoners were seen running down tho hill together and efter a smart chase they were captured two by the of ecer and Harcourt by a gentleman in a trap The of ecer saw money passed between the pri g gonors and when at the station 58 104d was found upon them A warder from Cold bth ﬁelds prison proved previous convictions against Wallace They were committed for trial\n",
      "Passage Score: 18.116172507235007\n",
      "Source Image ID: 3200797037\n",
      "Image Path: dataset\\img\\3200797037.jpg\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"A PROMINENT speaker at a free trader’s meeting at Madrid\"\n",
    "tokenized_query = query.split(\" \")\n",
    "\n",
    "scores = bm25.get_scores(tokenized_query)\n",
    "\n",
    "top_n = 3\n",
    "top_passages_idx = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:top_n]\n",
    "\n",
    "print(f\"Top {top_n} Passages (based on BM25 scores):\")\n",
    "for idx in top_passages_idx:\n",
    "    passage = ' '.join(tokenized_extracted_text_list[idx])  # Join tokens back into a string\n",
    "    image_id, passage_id ,image_path = image_info_list[idx]  # Get the image id and path for the passage\n",
    "    print(f\"Passage: {passage}\")\n",
    "    print(f'Passage Score: {scores[idx]}')\n",
    "    print(f\"Source Image ID: {image_id}\")\n",
    "    print(f\"Image Path: {image_path}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
